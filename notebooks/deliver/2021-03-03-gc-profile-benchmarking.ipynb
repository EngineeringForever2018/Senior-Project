{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "special-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pdpipe as pdp\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = Path('..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "from notebooks.utils import init_data_dir, extract_author_texts  # noqa\n",
    "\n",
    "from notebooks import pipes\n",
    "from notebooks.profiles import EuclideanProfile, NaiveBayesProfile\n",
    "from notebooks import benchmarking as bench\n",
    "from notebooks.feature_extractors import HeuristicsExtractor, FunctionWordCounter, POS2GramCounter\n",
    "from notebooks.thresholders import SimpleAccuracyThresholder, SimpleThresholder\n",
    "\n",
    "init_data_dir(project_root)\n",
    "\n",
    "preprocess_path = join(project_root, Path('data/preprocess'))\n",
    "outputs_path = join(project_root, 'outputs')\n",
    "\n",
    "train_df = pd.read_hdf(join(preprocess_path, 'bawe_train_sentences.hdf5'))\n",
    "valid_df = pd.read_hdf(join(preprocess_path, 'bawe_valid_sentences.hdf5'))\n",
    "\n",
    "train_df = train_df.rename(columns={\"sentence\": \"text\"})\n",
    "valid_df = valid_df.rename(columns={\"sentence\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractors = [(HeuristicsExtractor(), \"heuristics_extractor\")]\n",
    "feature_extractors = [(POS2GramCounter(), \"pos2gram_counter\"), (FunctionWordCounter(), \"function_word_counter\")]\n",
    "\n",
    "profiles = [(EuclideanProfile(), \"euclidean_distance_profile\")]\n",
    "# profiles = [(NaiveBayesProfile(), \"naive_bayes_profile\")]\n",
    "\n",
    "thresholders = [(SimpleThresholder(bench.balanced_accuracies), \"balanced_accuracy_thresholder\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hindu-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train dataset for pos2gram_counter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 238088/238088 [24:22<00:00, 162.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing valid dataset for pos2gram_counter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 58833/58833 [06:35<00:00, 148.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train dataset for function_word_counter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 238088/238088 [00:46<00:00, 5104.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing valid dataset for function_word_counter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 58833/58833 [00:11<00:00, 5170.17it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dfs = []\n",
    "\n",
    "for feature_extractor, display_name in feature_extractors:\n",
    "    train_path = join(preprocess_path, f\"bawe_train_preprocessed_{display_name}.hdf5\")\n",
    "    valid_path = join(preprocess_path, f\"bawe_valid_preprocessed_{display_name}.hdf5\")\n",
    "\n",
    "    preprocessed_train_exists = os.path.exists(train_path)\n",
    "    preprocessed_valid_exists = os.path.exists(valid_path)\n",
    "\n",
    "    if not (preprocessed_train_exists and preprocessed_valid_exists):\n",
    "        print(f\"Preprocessing train dataset for {display_name}\", flush=True)\n",
    "        preprocessed_train_df = feature_extractor(train_df, show_loading=True)\n",
    "        print(f\"Preprocessing valid dataset for {display_name}\", flush=True)\n",
    "        preprocessed_valid_df = feature_extractor(valid_df, show_loading=True)\n",
    "\n",
    "        preprocessed_train_df.to_hdf(train_path, key=f\"bawe_train_preprocessed_{display_name}\")\n",
    "        preprocessed_valid_df.to_hdf(valid_path, key=f\"bawe_valid_preprocessed_{display_name}\")\n",
    "    else:\n",
    "        preprocessed_train_df = pd.read_hdf(train_path)\n",
    "        preprocessed_valid_df = pd.read_hdf(valid_path)\n",
    "\n",
    "    preprocessed_dfs.append((preprocessed_train_df, preprocessed_valid_df, display_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exotic-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_train, function_valid, _ = preprocessed_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "removed-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_var = function_train.var()\n",
    "\n",
    "average_author_var = function_train.groupby(level=\"author\").var().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "muslim-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = ((overall_var - average_author_var) / overall_var) > 0\n",
    "\n",
    "actual_words = useful_features[useful_features].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "behavioral-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_function_train, better_function_valid = function_train[actual_words], function_valid[actual_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afraid-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_threshold(profile, df, thresholder):\n",
    "    author_set = set(df.index.get_level_values(0))\n",
    "\n",
    "    print(\"Training...\", flush=True)\n",
    "    distance_sets = []\n",
    "    true_flag_sets = []\n",
    "    for author in tqdm(author_set):\n",
    "        profile.reset()\n",
    "\n",
    "        author_texts, rest_df = extract_author_texts(author, df)\n",
    "        profile.feed(author_texts)\n",
    "        distances = profile.distances(rest_df)\n",
    "\n",
    "        true_flags = distances.index.get_level_values(0) != author\n",
    "\n",
    "        distance_sets.append(distances.to_numpy())\n",
    "        true_flag_sets.append(true_flags)\n",
    "\n",
    "    distances = np.concatenate(distance_sets)\n",
    "    true_flags = np.concatenate(true_flag_sets)\n",
    "\n",
    "    return thresholder(distances, true_flags)\n",
    "\n",
    "\n",
    "def test_profile(profile, threshold, df):\n",
    "    author_set = set(df.index.get_level_values(0))\n",
    "\n",
    "    print(\"Testing...\", flush=True)\n",
    "    flag_sets = []\n",
    "    true_flag_sets = []\n",
    "    for author in tqdm(author_set):\n",
    "        profile.reset()\n",
    "\n",
    "        author_texts, rest_df = extract_author_texts(author, df)\n",
    "        profile.feed(author_texts)\n",
    "        distances = profile.distances(rest_df)\n",
    "\n",
    "        flags = distances > threshold\n",
    "        true_flags = distances.index.get_level_values(0) != author\n",
    "\n",
    "        flag_sets.append(flags.to_numpy())\n",
    "        true_flag_sets.append(true_flags)\n",
    "\n",
    "    flags = np.concatenate(flag_sets)\n",
    "    true_flags = np.concatenate(true_flag_sets)\n",
    "\n",
    "    return [bench.balanced_accuracy(flags, true_flags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "golden-blowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527/527 [05:37<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold = train_threshold(EuclideanProfile(), better_function_train, SimpleThresholder(bench.balanced_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "wooden-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521/521 [05:33<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = test_profile(EuclideanProfile(), threshold, better_function_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "scheduled-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.5210756])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "unlikely-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_function_train = function_train[actual_words]\n",
    "\n",
    "chosen_author_texts = better_function_train.loc[(6212,)]\n",
    "\n",
    "author_means = function_train[actual_words].groupby(level=\"author\").mean()\n",
    "other_author_means = author_means.drop(index=6212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "concerned-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>7</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>41</th>\n",
       "      <th>43</th>\n",
       "      <th>...</th>\n",
       "      <th>72</th>\n",
       "      <th>79</th>\n",
       "      <th>82</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>98</th>\n",
       "      <th>103</th>\n",
       "      <th>106</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.567308</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>1.903846</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.640777</td>\n",
       "      <td>1.242718</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.087379</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.359223</td>\n",
       "      <td>0.019417</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>1.223301</td>\n",
       "      <td>0.592233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.310680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.396896</td>\n",
       "      <td>0.494457</td>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.108647</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.053215</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.037694</td>\n",
       "      <td>0.013304</td>\n",
       "      <td>0.472284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015521</td>\n",
       "      <td>1.492239</td>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.026608</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.013304</td>\n",
       "      <td>0.066519</td>\n",
       "      <td>0.044346</td>\n",
       "      <td>0.095344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.990244</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>0.297561</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.039024</td>\n",
       "      <td>0.029268</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.531707</td>\n",
       "      <td>0.039024</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.882927</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.474359</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.371795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6210</th>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.659091</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6211</th>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.373077</td>\n",
       "      <td>0.050769</td>\n",
       "      <td>0.135385</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.054615</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.048462</td>\n",
       "      <td>0.097692</td>\n",
       "      <td>0.353077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.528462</td>\n",
       "      <td>0.025385</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.150769</td>\n",
       "      <td>0.082308</td>\n",
       "      <td>0.063846</td>\n",
       "      <td>0.013077</td>\n",
       "      <td>0.111538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6214</th>\n",
       "      <td>0.334963</td>\n",
       "      <td>0.594132</td>\n",
       "      <td>0.141809</td>\n",
       "      <td>0.110024</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.760391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>1.767726</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.019560</td>\n",
       "      <td>0.542787</td>\n",
       "      <td>0.239609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6215</th>\n",
       "      <td>0.541850</td>\n",
       "      <td>0.792952</td>\n",
       "      <td>0.125551</td>\n",
       "      <td>0.240088</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063877</td>\n",
       "      <td>0.568282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.409692</td>\n",
       "      <td>0.026432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.482379</td>\n",
       "      <td>0.191630</td>\n",
       "      <td>0.033040</td>\n",
       "      <td>0.182819</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.136564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>0.295652</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.078261</td>\n",
       "      <td>0.208696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.513043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>1.243478</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.147826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         7         12        13        32        33        37   \\\n",
       "author                                                                         \n",
       "1       0.567308  0.971154  0.096154  0.163462  0.048077  0.163462  0.000000   \n",
       "2       0.640777  1.242718  0.126214  0.194175  0.058252  0.087379  0.058252   \n",
       "3       0.396896  0.494457  0.039911  0.108647  0.002217  0.053215  0.011086   \n",
       "4       0.463415  0.990244  0.009756  0.297561  0.014634  0.014634  0.004878   \n",
       "5       0.435897  0.474359  0.044872  0.064103  0.000000  0.012821  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "6210    0.647727  0.528409  0.102273  0.125000  0.022727  0.005682  0.000000   \n",
       "6211    0.292308  0.373077  0.050769  0.135385  0.003846  0.054615  0.030000   \n",
       "6214    0.334963  0.594132  0.141809  0.110024  0.012225  0.034230  0.000000   \n",
       "6215    0.541850  0.792952  0.125551  0.240088  0.017621  0.024229  0.000000   \n",
       "6998    0.295652  0.417391  0.078261  0.208696  0.000000  0.000000  0.000000   \n",
       "\n",
       "             38        41        43   ...       72        79        82   \\\n",
       "author                                ...                                 \n",
       "1       0.000000  0.134615  0.692308  ...  0.048077  1.903846  0.019231   \n",
       "2       0.000000  0.000000  0.893204  ...  0.000000  2.359223  0.019417   \n",
       "3       0.037694  0.013304  0.472284  ...  0.015521  1.492239  0.011086   \n",
       "4       0.039024  0.029268  0.658537  ...  0.000000  2.531707  0.039024   \n",
       "5       0.019231  0.032051  0.448718  ...  0.000000  1.371795  0.000000   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "6210    0.005682  0.017045  0.312500  ...  0.000000  1.659091  0.090909   \n",
       "6211    0.048462  0.097692  0.353077  ...  0.010000  1.528462  0.025385   \n",
       "6214    0.000000  0.012225  0.760391  ...  0.004890  1.767726  0.012225   \n",
       "6215    0.000000  0.063877  0.568282  ...  0.000000  1.409692  0.026432   \n",
       "6998    0.469565  0.043478  0.513043  ...  0.008696  1.243478  0.017391   \n",
       "\n",
       "             90        91        95        96        98        103       106  \n",
       "author                                                                        \n",
       "1       0.000000  0.711538  0.288462  0.000000  0.480769  0.000000  0.201923  \n",
       "2       0.106796  1.223301  0.592233  0.000000  0.417476  0.009709  0.310680  \n",
       "3       0.026608  0.439024  0.133038  0.013304  0.066519  0.044346  0.095344  \n",
       "4       0.004878  0.882927  0.219512  0.000000  0.112195  0.000000  0.141463  \n",
       "5       0.025641  0.442308  0.051282  0.044872  0.012821  0.000000  0.102564  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "6210    0.000000  0.494318  0.409091  0.005682  0.017045  0.000000  0.215909  \n",
       "6211    0.003846  0.423077  0.150769  0.082308  0.063846  0.013077  0.111538  \n",
       "6214    0.019560  0.542787  0.239609  0.000000  0.207824  0.000000  0.134474  \n",
       "6215    0.000000  0.482379  0.191630  0.033040  0.182819  0.002203  0.136564  \n",
       "6998    0.000000  0.904348  0.052174  0.000000  0.026087  0.069565  0.147826  \n",
       "\n",
       "[526 rows x 27 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_author_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "digital-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3417540749586829"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_author_means = chosen_author_texts.groupby(level=\"text_id\").mean()\n",
    "chosen_author_means\n",
    "np.linalg.norm(chosen_author_means.iloc[0] - chosen_author_means.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "instructional-darwin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.046756212038611"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.linalg.norm(chosen_author_means.iloc[0] - other_author_means, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informative-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heuristics_train = preprocessed_dfs[0][0]\n",
    "# heuristics_test = preprocessed_dfs[0][1]\n",
    "\n",
    "# scaled_heuristics_train = (heuristics_train - heuristics_train.mean()) / heuristics_train.std()\n",
    "# scaled_heuristics_test = (heuristics_test - heuristics_test.mean()) / heuristics_test.std()\n",
    "# preprocessed_dfs.append((scaled_heuristics_train, scaled_heuristics_test, \"scaled_heuristics\"))\n",
    "\n",
    "# function_words_train = preprocessed_dfs[0][0]\n",
    "# function_words_test = preprocessed_dfs[0][1]\n",
    "# pca_train = function_words_train\n",
    "\n",
    "# pca_standardized = (pca_train - pca_train.mean()) / pca_train.std()\n",
    "\n",
    "# pca_cov = pca_standardized.cov()\n",
    "\n",
    "# pca_eigvals, pca_eigvecs = np.linalg.eig(pca_cov)\n",
    "# sort_indices = np.flip(np.argsort(pca_eigvals))\n",
    "# pca_eigvals, pca_eigvecs = pca_eigvals[sort_indices], pca_eigvecs[sort_indices]\n",
    "\n",
    "# transformation_matrix0 = pca_eigvecs[:, :5]\n",
    "# transformation_matrix1 = pca_eigvecs[:, :10]\n",
    "# transformation_matrix2 = pca_eigvecs[:, :15]\n",
    "# transformation_matrix3 = pca_eigvecs[:, :20]\n",
    "\n",
    "# pca_function_words_train0 = function_words_train.dot(transformation_matrix0)\n",
    "# pca_function_words_test0 = function_words_test.dot(transformation_matrix0)\n",
    "# pca_function_words_train1 = function_words_train.dot(transformation_matrix1)\n",
    "# pca_function_words_test1 = function_words_test.dot(transformation_matrix1)\n",
    "# pca_function_words_train2 = function_words_train.dot(transformation_matrix2)\n",
    "# pca_function_words_test2 = function_words_test.dot(transformation_matrix2)\n",
    "# pca_function_words_train3 = function_words_train.dot(transformation_matrix3)\n",
    "# pca_function_words_test3 = function_words_test.dot(transformation_matrix3)\n",
    "\n",
    "# preprocessed_dfs.append((pca_function_words_train0, pca_function_words_test0, \"pca_function_words0\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train1, pca_function_words_test1, \"pca_function_words1\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train2, pca_function_words_test2, \"pca_function_words2\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train3, pca_function_words_test3, \"pca_function_words3\"))\n",
    "\n",
    "# function_words_train = preprocessed_dfs[0][0]\n",
    "# function_words_test = preprocessed_dfs[0][1]\n",
    "# pca_train = function_words_train\n",
    "\n",
    "# pca_standardized = (pca_train - pca_train.mean()) / pca_train.std()\n",
    "\n",
    "# pca_cov = pca_standardized.cov()\n",
    "\n",
    "# pca_eigvals, pca_eigvecs = np.linalg.eig(pca_cov)\n",
    "# sort_indices = np.flip(np.argsort(pca_eigvals))\n",
    "# pca_eigvals, pca_eigvecs = pca_eigvals[sort_indices], pca_eigvecs[sort_indices]\n",
    "\n",
    "# transformation_matrix0 = pca_eigvecs[:, :5]\n",
    "# transformation_matrix1 = pca_eigvecs[:, :10]\n",
    "# transformation_matrix2 = pca_eigvecs[:, :15]\n",
    "# transformation_matrix3 = pca_eigvecs[:, :20]\n",
    "\n",
    "# pca_function_words_train0 = function_words_train.dot(transformation_matrix0)\n",
    "# pca_function_words_test0 = function_words_test.dot(transformation_matrix0)\n",
    "# pca_function_words_train1 = function_words_train.dot(transformation_matrix1)\n",
    "# pca_function_words_test1 = function_words_test.dot(transformation_matrix1)\n",
    "# pca_function_words_train2 = function_words_train.dot(transformation_matrix2)\n",
    "# pca_function_words_test2 = function_words_test.dot(transformation_matrix2)\n",
    "# pca_function_words_train3 = function_words_train.dot(transformation_matrix3)\n",
    "# pca_function_words_test3 = function_words_test.dot(transformation_matrix3)\n",
    "\n",
    "# preprocessed_dfs.append((pca_function_words_train0, pca_function_words_test0, \"pca_function_words0\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train1, pca_function_words_test1, \"pca_function_words1\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train2, pca_function_words_test2, \"pca_function_words2\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train3, pca_function_words_test3, \"pca_function_words3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brown-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 527/527 [07:08<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 521/521 [07:03<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_threshold(profile, df, thresholder):\n",
    "    author_set = set(df.index.get_level_values(0))\n",
    "\n",
    "    print(\"Training...\", flush=True)\n",
    "    distance_sets = []\n",
    "    true_flag_sets = []\n",
    "    for author in tqdm(author_set):\n",
    "        profile.reset()\n",
    "\n",
    "        author_texts, rest_df = extract_author_texts(author, df)\n",
    "        profile.feed(author_texts)\n",
    "        distances = profile.distances(rest_df)\n",
    "\n",
    "        true_flags = distances.index.get_level_values(0) != author\n",
    "\n",
    "        distance_sets.append(distances.to_numpy())\n",
    "        true_flag_sets.append(true_flags)\n",
    "\n",
    "    distances = np.concatenate(distance_sets)\n",
    "    true_flags = np.concatenate(true_flag_sets)\n",
    "\n",
    "    return thresholder(distances, true_flags)\n",
    "\n",
    "\n",
    "def test_profile(profile, threshold, df):\n",
    "    author_set = set(df.index.get_level_values(0))\n",
    "\n",
    "    print(\"Testing...\", flush=True)\n",
    "    flag_sets = []\n",
    "    true_flag_sets = []\n",
    "    for author in tqdm(author_set):\n",
    "        profile.reset()\n",
    "\n",
    "        author_texts, rest_df = extract_author_texts(author, df)\n",
    "        profile.feed(author_texts)\n",
    "        distances = profile.distances(rest_df)\n",
    "\n",
    "        flags = distances > threshold\n",
    "        true_flags = distances.index.get_level_values(0) != author\n",
    "\n",
    "        flag_sets.append(flags.to_numpy())\n",
    "        true_flag_sets.append(true_flags)\n",
    "\n",
    "    flags = np.concatenate(flag_sets)\n",
    "    true_flags = np.concatenate(true_flag_sets)\n",
    "\n",
    "    return [bench.balanced_accuracy(flags, true_flags)]\n",
    "\n",
    "\n",
    "score_data = []\n",
    "thresholds = []\n",
    "model_names = []\n",
    "\n",
    "for profile, profile_name in profiles:\n",
    "    for thresholder, thresholder_name in thresholders:\n",
    "        for preprocessed_train_df, preprocessed_valid_df, extractor_name in preprocessed_dfs:\n",
    "            threshold = train_threshold(profile, preprocessed_train_df, thresholder)\n",
    "            thresholds.append(threshold)\n",
    "            profile.reset()\n",
    "\n",
    "            scores = test_profile(pr\n",
    "                                  ofile, threshold, preprocessed_valid_df)\n",
    "            score_data.append(scores)\n",
    "            model_names.append(f\"{profile_name}-{thresholder_name}-{extractor_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "secondary-color",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[array([0.49067909])]],\n",
       " ['euclidean_distance_profile-balanced_accuracy_thresholder-pos2gram_counter'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_data, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "responsible-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_profile(EuclideanProfile(), thresholds[0], preprocessed_dfs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "known-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_count = len(function_words_train)\n",
    "\n",
    "# where_true = ((function_words_train.sum() / (sentence_count / 100)) > 1)\n",
    "\n",
    "# chosen_word_indices = where_true[where_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "macro-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../notebooks/resources/original_function_words.txt\") as f:\n",
    "#     words = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dominant-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_words = [words[chosen_index] for chosen_index in chosen_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "composite-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../notebooks/resources/filtered_function_words.txt\", \"w\") as f:\n",
    "#     f.writelines(chosen_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-shelter",
   "metadata": {},
   "source": [
    "The threshold may be overfitting to individual author texts, not the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "marine-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_data = np.concatenate([results[None, :] for results in score_data])\n",
    "\n",
    "# results_df = pd.DataFrame(np.array(score_data), index=model_names, columns=[\"balanced_accuracy\"])\n",
    "\n",
    "# results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
