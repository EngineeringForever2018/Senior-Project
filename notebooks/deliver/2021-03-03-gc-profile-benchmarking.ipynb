{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "returning-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pdpipe as pdp\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = Path('..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "from notebooks.utils import init_data_dir, extract_author_texts  # noqa\n",
    "\n",
    "from notebooks import pipes\n",
    "from notebooks.profiles import EuclideanProfile, NaiveBayesProfile\n",
    "from notebooks import benchmarking as bench\n",
    "from notebooks.feature_extractors import HeuristicsExtractor, FunctionWordCounter, POS2GramCounter\n",
    "from notebooks.thresholders import SimpleAccuracyThresholder, SimpleThresholder\n",
    "\n",
    "init_data_dir(project_root)\n",
    "\n",
    "preprocess_path = join(project_root, Path('data/preprocess'))\n",
    "outputs_path = join(project_root, 'outputs')\n",
    "\n",
    "train_df = pd.read_hdf(join(preprocess_path, 'bawe_train_sentences.hdf5'))\n",
    "valid_df = pd.read_hdf(join(preprocess_path, 'bawe_valid_sentences.hdf5'))\n",
    "\n",
    "train_df = train_df.rename(columns={\"sentence\": \"text\"})\n",
    "valid_df = valid_df.rename(columns={\"sentence\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vanilla-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractors = [(HeuristicsExtractor(), \"heuristics_extractor\")]\n",
    "feature_extractors = [(POS2GramCounter(), \"pos2gram_counter\")]\n",
    "\n",
    "profiles = [(EuclideanProfile(), \"euclidean_distance_profile\")]\n",
    "# profiles = [(NaiveBayesProfile(), \"naive_bayes_profile\")]\n",
    "\n",
    "thresholders = [(SimpleThresholder(bench.balanced_accuracies), \"balanced_accuracy_thresholder\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "north-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dfs = []\n",
    "\n",
    "for feature_extractor, display_name in feature_extractors:\n",
    "    train_path = join(preprocess_path, f\"bawe_train_preprocessed_{display_name}.hdf5\")\n",
    "    valid_path = join(preprocess_path, f\"bawe_valid_preprocessed_{display_name}.hdf5\")\n",
    "\n",
    "    preprocessed_train_exists = os.path.exists(train_path)\n",
    "    preprocessed_valid_exists = os.path.exists(valid_path)\n",
    "\n",
    "    if not (preprocessed_train_exists and preprocessed_valid_exists):\n",
    "        print(f\"Preprocessing train dataset for {display_name}\", flush=True)\n",
    "        preprocessed_train_df = feature_extractor(train_df, show_loading=True)\n",
    "        print(f\"Preprocessing valid dataset for {display_name}\", flush=True)\n",
    "        preprocessed_valid_df = feature_extractor(valid_df, show_loading=True)\n",
    "\n",
    "        preprocessed_train_df.to_hdf(train_path, key=f\"bawe_train_preprocessed_{display_name}\")\n",
    "        preprocessed_valid_df.to_hdf(valid_path, key=f\"bawe_valid_preprocessed_{display_name}\")\n",
    "    else:\n",
    "        preprocessed_train_df = pd.read_hdf(train_path)\n",
    "        preprocessed_valid_df = pd.read_hdf(valid_path)\n",
    "\n",
    "    preprocessed_dfs.append((preprocessed_train_df, preprocessed_valid_df, display_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polar-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heuristics_train = preprocessed_dfs[0][0]\n",
    "# heuristics_test = preprocessed_dfs[0][1]\n",
    "\n",
    "# scaled_heuristics_train = (heuristics_train - heuristics_train.mean()) / heuristics_train.std()\n",
    "# scaled_heuristics_test = (heuristics_test - heuristics_test.mean()) / heuristics_test.std()\n",
    "# preprocessed_dfs.append((scaled_heuristics_train, scaled_heuristics_test, \"scaled_heuristics\"))\n",
    "\n",
    "# function_words_train = preprocessed_dfs[0][0]\n",
    "# function_words_test = preprocessed_dfs[0][1]\n",
    "# pca_train = function_words_train\n",
    "\n",
    "# pca_standardized = (pca_train - pca_train.mean()) / pca_train.std()\n",
    "\n",
    "# pca_cov = pca_standardized.cov()\n",
    "\n",
    "# pca_eigvals, pca_eigvecs = np.linalg.eig(pca_cov)\n",
    "# sort_indices = np.flip(np.argsort(pca_eigvals))\n",
    "# pca_eigvals, pca_eigvecs = pca_eigvals[sort_indices], pca_eigvecs[sort_indices]\n",
    "\n",
    "# transformation_matrix0 = pca_eigvecs[:, :5]\n",
    "# transformation_matrix1 = pca_eigvecs[:, :10]\n",
    "# transformation_matrix2 = pca_eigvecs[:, :15]\n",
    "# transformation_matrix3 = pca_eigvecs[:, :20]\n",
    "\n",
    "# pca_function_words_train0 = function_words_train.dot(transformation_matrix0)\n",
    "# pca_function_words_test0 = function_words_test.dot(transformation_matrix0)\n",
    "# pca_function_words_train1 = function_words_train.dot(transformation_matrix1)\n",
    "# pca_function_words_test1 = function_words_test.dot(transformation_matrix1)\n",
    "# pca_function_words_train2 = function_words_train.dot(transformation_matrix2)\n",
    "# pca_function_words_test2 = function_words_test.dot(transformation_matrix2)\n",
    "# pca_function_words_train3 = function_words_train.dot(transformation_matrix3)\n",
    "# pca_function_words_test3 = function_words_test.dot(transformation_matrix3)\n",
    "\n",
    "# preprocessed_dfs.append((pca_function_words_train0, pca_function_words_test0, \"pca_function_words0\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train1, pca_function_words_test1, \"pca_function_words1\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train2, pca_function_words_test2, \"pca_function_words2\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train3, pca_function_words_test3, \"pca_function_words3\"))\n",
    "\n",
    "# function_words_train = preprocessed_dfs[0][0]\n",
    "# function_words_test = preprocessed_dfs[0][1]\n",
    "# pca_train = function_words_train\n",
    "\n",
    "# pca_standardized = (pca_train - pca_train.mean()) / pca_train.std()\n",
    "\n",
    "# pca_cov = pca_standardized.cov()\n",
    "\n",
    "# pca_eigvals, pca_eigvecs = np.linalg.eig(pca_cov)\n",
    "# sort_indices = np.flip(np.argsort(pca_eigvals))\n",
    "# pca_eigvals, pca_eigvecs = pca_eigvals[sort_indices], pca_eigvecs[sort_indices]\n",
    "\n",
    "# transformation_matrix0 = pca_eigvecs[:, :5]\n",
    "# transformation_matrix1 = pca_eigvecs[:, :10]\n",
    "# transformation_matrix2 = pca_eigvecs[:, :15]\n",
    "# transformation_matrix3 = pca_eigvecs[:, :20]\n",
    "\n",
    "# pca_function_words_train0 = function_words_train.dot(transformation_matrix0)\n",
    "# pca_function_words_test0 = function_words_test.dot(transformation_matrix0)\n",
    "# pca_function_words_train1 = function_words_train.dot(transformation_matrix1)\n",
    "# pca_function_words_test1 = function_words_test.dot(transformation_matrix1)\n",
    "# pca_function_words_train2 = function_words_train.dot(transformation_matrix2)\n",
    "# pca_function_words_test2 = function_words_test.dot(transformation_matrix2)\n",
    "# pca_function_words_train3 = function_words_train.dot(transformation_matrix3)\n",
    "# pca_function_words_test3 = function_words_test.dot(transformation_matrix3)\n",
    "\n",
    "# preprocessed_dfs.append((pca_function_words_train0, pca_function_words_test0, \"pca_function_words0\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train1, pca_function_words_test1, \"pca_function_words1\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train2, pca_function_words_test2, \"pca_function_words2\"))\n",
    "# preprocessed_dfs.append((pca_function_words_train3, pca_function_words_test3, \"pca_function_words3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inner-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 527/527 [07:08<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 521/521 [07:03<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_threshold(profile, df, thresholder):\n",
    "    author_set = set(df.index.get_level_values(0))\n",
    "\n",
    "    print(\"Training...\", flush=True)\n",
    "    distance_sets = []\n",
    "    true_flag_sets = []\n",
    "    for author in tqdm(author_set):\n",
    "        profile.reset()\n",
    "\n",
    "        author_texts, rest_df = extract_author_texts(author, df)\n",
    "        profile.feed(author_texts)\n",
    "        distances = profile.distances(rest_df)\n",
    "\n",
    "        true_flags = distances.index.get_level_values(0) != author\n",
    "\n",
    "        distance_sets.append(distances.to_numpy())\n",
    "        true_flag_sets.append(true_flags)\n",
    "\n",
    "    distances = np.concatenate(distance_sets)\n",
    "    true_flags = np.concatenate(true_flag_sets)\n",
    "\n",
    "    return thresholder(distances, true_flags)\n",
    "\n",
    "\n",
    "def test_profile(profile, threshold, df):\n",
    "    author_set = set(df.index.get_level_values(0))\n",
    "\n",
    "    print(\"Testing...\", flush=True)\n",
    "    flag_sets = []\n",
    "    true_flag_sets = []\n",
    "    for author in tqdm(author_set):\n",
    "        profile.reset()\n",
    "\n",
    "        author_texts, rest_df = extract_author_texts(author, df)\n",
    "        profile.feed(author_texts)\n",
    "        distances = profile.distances(rest_df)\n",
    "\n",
    "        flags = distances > threshold\n",
    "        true_flags = distances.index.get_level_values(0) != author\n",
    "\n",
    "        flag_sets.append(flags.to_numpy())\n",
    "        true_flag_sets.append(true_flags)\n",
    "\n",
    "    flags = np.concatenate(flag_sets)\n",
    "    true_flags = np.concatenate(true_flag_sets)\n",
    "\n",
    "    return [bench.balanced_accuracy(flags, true_flags)]\n",
    "\n",
    "\n",
    "score_data = []\n",
    "thresholds = []\n",
    "model_names = []\n",
    "\n",
    "for profile, profile_name in profiles:\n",
    "    for thresholder, thresholder_name in thresholders:\n",
    "        for preprocessed_train_df, preprocessed_valid_df, extractor_name in preprocessed_dfs:\n",
    "            threshold = train_threshold(profile, preprocessed_train_df, thresholder)\n",
    "            thresholds.append(threshold)\n",
    "            profile.reset()\n",
    "\n",
    "            scores = test_profile(pr\n",
    "                                  ofile, threshold, preprocessed_valid_df)\n",
    "            score_data.append(scores)\n",
    "            model_names.append(f\"{profile_name}-{thresholder_name}-{extractor_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "physical-activity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[array([0.49067909])]],\n",
       " ['euclidean_distance_profile-balanced_accuracy_thresholder-pos2gram_counter'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_data, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_profile(EuclideanProfile(), thresholds[0], preprocessed_dfs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "technological-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_count = len(function_words_train)\n",
    "\n",
    "# where_true = ((function_words_train.sum() / (sentence_count / 100)) > 1)\n",
    "\n",
    "# chosen_word_indices = where_true[where_true].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "informal-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../notebooks/resources/original_function_words.txt\") as f:\n",
    "#     words = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "monthly-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_words = [words[chosen_index] for chosen_index in chosen_word_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outdoor-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../notebooks/resources/filtered_function_words.txt\", \"w\") as f:\n",
    "#     f.writelines(chosen_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-competition",
   "metadata": {},
   "source": [
    "The threshold may be overfitting to individual author texts, not the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "quality-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_data = np.concatenate([results[None, :] for results in score_data])\n",
    "\n",
    "# results_df = pd.DataFrame(np.array(score_data), index=model_names, columns=[\"balanced_accuracy\"])\n",
    "\n",
    "# results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:avpd]",
   "language": "python",
   "name": "conda-env-avpd-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
