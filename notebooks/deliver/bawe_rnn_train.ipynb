{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfied-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import spacy\n",
    "import sys\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = Path('..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "from notebooks.utils import init_data_dir  # noqa\n",
    "\n",
    "from notebooks import pipes  # noqa\n",
    "from notebooks.datatools import AuthorDataset, EqualOpDataLoader  # noqa\n",
    "from notebooks.nets import EuclideanDiscriminator, PackedEmbedder, StyleEncoder, Seq2Vec  # noqa\n",
    "from notebooks.utils import POSVocab  # noqa\n",
    "\n",
    "init_data_dir(project_root)\n",
    "\n",
    "preprocess_path = join(project_root, Path('data/preprocess'))\n",
    "\n",
    "dev = torch.device(0)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "pos_vocab = POSVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "persistent-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_dir = join(project_root, 'runs')\n",
    "\n",
    "writer = SummaryWriter(join(writer_dir, f'{datetime.now()}-bawe-par-encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "higher-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "reprocess = False\n",
    "\n",
    "train_data_path = join(preprocess_path, 'bawe_train_sentences_tokenized.hdf5')\n",
    "valid_data_path = join(preprocess_path, 'bawe_valid_sentences_tokenized.hdf5')\n",
    "\n",
    "train_data_exists = os.path.exists(train_data_path)\n",
    "valid_data_exists = os.path.exists(valid_data_path)\n",
    "\n",
    "pipeline = pipes.POSTokenize(nlp=nlp, pos_vocab=pos_vocab, show_loading=True)\n",
    "\n",
    "if not (train_data_exists and valid_data_exists) or reprocess:\n",
    "    print('Processing...', flush=True)\n",
    "\n",
    "    train_df = pd.read_hdf(join(preprocess_path, 'bawe_train_sentences.hdf5'))\n",
    "    valid_df = pd.read_hdf(join(preprocess_path, 'bawe_valid_sentences.hdf5'))\n",
    "\n",
    "    train_data = pipeline(train_df)\n",
    "    valid_data = pipeline(valid_df)\n",
    "\n",
    "    train_data.to_hdf(train_data_path, 'bawe_train_sentences_tokenized')\n",
    "    valid_data.to_hdf(valid_data_path, 'bawe_valid_sentences_tokenized')\n",
    "else:\n",
    "    train_data = pd.read_hdf(train_data_path)\n",
    "    valid_data = pd.read_hdf(valid_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organizational-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence           [8, 2, 10, 3, 8, 10, 5, 12, 18, 8, 10, 3, 15, ...\n",
       "sentence_length                                                   71\n",
       "Name: (28, 0, 1), dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[(28, 0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unable-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nominated-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipes.GroupSentences(n=num_sentences)\n",
    "\n",
    "train_data = pipeline(train_data)\n",
    "valid_data = pipeline(valid_data)\n",
    "\n",
    "train_set = AuthorDataset(train_data)\n",
    "valid_set = AuthorDataset(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "invisible-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = PackedEmbedder(Embedding(len(pos_vocab), 10,\n",
    "                                    padding_idx=pos_vocab['<pad>']))\n",
    "sentence_encoder = Seq2Vec(LSTM(10, 5))\n",
    "\n",
    "style_encoder = StyleEncoder(embedder, sentence_encoder).to(dev)\n",
    "style_discriminator = EuclideanDiscriminator(n=num_sentences).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statewide-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.seed()\n",
    "\n",
    "# Hyperparameters\n",
    "batch_count = 1000\n",
    "lr = 1e-6\n",
    "opt = optim.SGD([{'params': style_discriminator.parameters()},\n",
    "                 {'params': style_encoder.parameters()}], lr=lr)\n",
    "criterion = mse_loss\n",
    "bs = 75\n",
    "\n",
    "pipeline = pipes.PackSequence(dev=dev)\n",
    "train_dl = EqualOpDataLoader(train_set, bs=bs, pipeline=pipeline)\n",
    "valid_dl = EqualOpDataLoader(valid_set, bs=bs, pipeline=pipeline)\n",
    "\n",
    "\n",
    "def fit(validate=True, validate_every=100):\n",
    "    train_dl.batch_count = batch_count\n",
    "    for index, ((x1b, y1b), (x2b, y2b)) in tqdm(enumerate(train_dl),\n",
    "                                                total=len(train_dl)):\n",
    "        x1_encoding = style_encoder(x1b)\n",
    "        x2_encoding = style_encoder(x2b)\n",
    "\n",
    "        pred = style_discriminator(x1_encoding, x2_encoding).squeeze(1)\n",
    "\n",
    "        yb = y_difference(y1b, y2b).to(dtype=torch.float)\n",
    "\n",
    "        loss = criterion(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        writer.add_scalar('Training Loss', loss, index)\n",
    "        writer.flush()\n",
    "\n",
    "        if validate:\n",
    "            if index % 100 == 0:\n",
    "                valid_loss, valid_acc = evaluate(valid_dl, give_acc=True)\n",
    "                writer.add_scalar('Validation Loss', valid_loss, index)\n",
    "                writer.add_scalar('Validation Accuracy', valid_acc, index)\n",
    "                writer.flush()\n",
    "\n",
    "\n",
    "def y_difference(y1, y2):\n",
    "    return torch.logical_not((y1 == y2)).to(dtype=int).to(dev)\n",
    "\n",
    "\n",
    "def evaluate(dl, give_acc=False):\n",
    "    with torch.no_grad():\n",
    "        preds_y = [(style_discriminator(style_encoder(x1b),\n",
    "                                        style_encoder(x2b)),\n",
    "                    y_difference(y1b, y2b))\n",
    "                   for (x1b, y1b), (x2b, y2b) in dl]\n",
    "\n",
    "        losses = [criterion(preds_b.squeeze(1), yb) for preds_b, yb in preds_y]\n",
    "        loss = sum(losses) / len(losses)\n",
    "\n",
    "        if give_acc:\n",
    "            accs = [accuracy(preds_b, yb) for preds_b, yb in preds_y]\n",
    "            acc = sum(accs) / len(accs)\n",
    "\n",
    "            return loss, acc\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def accuracy(out, y):\n",
    "    preds = out > 0.5\n",
    "    return (preds == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "driving-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|##4       | 241/1000 [04:11<13:12,  1.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;34m/home/gchristensen/anaconda3/envs/avpd/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m, in \u001B[0;32mrun_code\u001B[0m:\nLine \u001B[0;34m3427\u001B[0m:  exec(code_obj, \u001B[36mself\u001B[39;49;00m.user_global_ns, \u001B[36mself\u001B[39;49;00m.user_ns)\n",
      "In  \u001B[0;34m[9]\u001B[0m:\nLine \u001B[0;34m1\u001B[0m:     fit(validate=\u001B[34mFalse\u001B[39;49;00m)\n",
      "In  \u001B[0;34m[8]\u001B[0m:\nLine \u001B[0;34m19\u001B[0m:    total=\u001B[36mlen\u001B[39;49;00m(train_dl)):\n",
      "File \u001B[0;34m/home/gchristensen/anaconda3/envs/avpd/lib/python3.7/site-packages/tqdm/std.py\u001B[0m, in \u001B[0;32m__iter__\u001B[0m:\nLine \u001B[0;34m1170\u001B[0m:  \u001B[34mfor\u001B[39;49;00m obj \u001B[35min\u001B[39;49;00m iterable:\n",
      "File \u001B[0;34m/home/gchristensen/base/repos/avpd/notebooks/notebooks/datatools/_equal_op_dataloader.py\u001B[0m, in \u001B[0;32m__iter__\u001B[0m:\nLine \u001B[0;34m45\u001B[0m:    first_df = \u001B[36mself\u001B[39;49;00m.dataset.sample_authors(first_labels.tolist())\n",
      "File \u001B[0;34m/home/gchristensen/base/repos/avpd/notebooks/notebooks/datatools/_author_dataset.py\u001B[0m, in \u001B[0;32msample_authors\u001B[0m:\nLine \u001B[0;34m31\u001B[0m:    groups = \u001B[36mself\u001B[39;49;00m.df[\u001B[36mself\u001B[39;49;00m.df.index.droplevel([\u001B[33m'\u001B[39;49;00m\u001B[33mgroup_position\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m, \u001B[33m'\u001B[39;49;00m\u001B[33msentence_position\u001B[39;49;00m\u001B[33m'\u001B[39;49;00m]).isin(\u001B[36mzip\u001B[39;49;00m(authors, text_ids))]\n",
      "File \u001B[0;34m/home/gchristensen/anaconda3/envs/avpd/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001B[0m, in \u001B[0;32misin\u001B[0m:\nLine \u001B[0;34m3849\u001B[0m:  \u001B[34mreturn\u001B[39;49;00m algos.isin(\u001B[36mself\u001B[39;49;00m._values, values)\n",
      "File \u001B[0;34m/home/gchristensen/anaconda3/envs/avpd/lib/python3.7/site-packages/pandas/core/algorithms.py\u001B[0m, in \u001B[0;32misin\u001B[0m:\nLine \u001B[0;34m493\u001B[0m:   \u001B[34mreturn\u001B[39;49;00m f(comps, values)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: \n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m"
     ]
    }
   ],
   "source": [
    "fit(validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = join(project_root, 'outputs')\n",
    "if not os.path.isdir(outputs_dir):\n",
    "    os.mkdir(outputs_dir)\n",
    "\n",
    "torch.save(style_encoder.state_dict(),\n",
    "           join(outputs_dir, 'bawe_style_encoder_sd.pt'))\n",
    "torch.save(style_discriminator.state_dict(),\n",
    "           join(outputs_dir, 'bawe_style_discriminator_sd.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:avpd]",
   "language": "python",
   "name": "conda-env-avpd-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}