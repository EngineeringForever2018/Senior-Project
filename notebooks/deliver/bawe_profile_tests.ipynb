{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(Path('..')))\n",
    "\n",
    "from notebooks.profiles import MahalanobisProfile\n",
    "from notebooks.feature_extractors import POSPCAExtractor, HeuristicExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bawe_group_dir = Path('../data/preprocess/bawe-group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report 1\n",
    "\n",
    "This is a summary of all the work that has been done over winter break/the first few weeks of the semester.\n",
    "\n",
    "## Current Progress\n",
    "\n",
    "We currently use profile-based methods to determine whether a new text belongs to a given author. Given a series of texts for an author, we compute a profile for that author, and then make a decision about whether a new text is from that author based on the distance from the new text to the profile. The current profile implementation is based on finding the mean feature vector given the different feature vectors for a text, and then the mahalanobis distance between the mean and a feature vector for the new text is computed. We then find the probability that the new text belongs to the profile, and output that. In the below tests, a threshold of 90% was used to flag essays as either belonging to the same author or a different author. We currently have 3 different feature extractions methods being worked on:\n",
    "\n",
    "1. PCA based on POS bigram counts (good accuracy, currently best model)\n",
    "2. Heuristics gathering (chance accuracy, but may be able to boost performance of bigram counts)\n",
    "    * Will likely have better accuracy when we use all heuristics and gather more\n",
    "3. LSTM POS encoder (WIP)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Interpretability issues\n",
    "    * We would like to have some kind of interface to explain to the user why an essay is being flagged.\n",
    "    * Use interpretable model, but if probability is too close to threshold, use high performance model and prompt for review?\n",
    "    * Use high performance model, but display most relevant heuristics to aid in diagnosis\n",
    "        * Correlation does not equal causation?\n",
    "    * If nothing else, we can heavily encourage the user to review any essays that are flagged and make a decision for themselves.\n",
    "2. Are there any other feature extraction techniques we should try?\n",
    "3. Adversarial models?\n",
    "    * Train model to turn text from a different author into text that is accepted by the profile\n",
    "        * May aid in assessing whether the profiles are resilient to patch writing\n",
    "        * Concern: model may find a way to perfectly capture the style of an author and reform the text to that style, this new text *should* be accepted\n",
    "\n",
    "### Precision, Recall, and Accuracy on the BAWE data\n",
    "\n",
    "We used the British Academic Written English Corpus (BAWE) to test the precision, recall, and accuracy of our profilers. Five authors are randomly selected from the set, and then they are compared with 20 random different authors. The first essay of an author is used to \"profile\" them, and then their essays along with the essays from the different authors are tested against this profile. The profile should return a high probability for essays that belong to the profile and a low probability for essays that don't. For this demo, a cutoff of 90% is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_profile(profile, cutoff):\n",
    "    ids = os.listdir(bawe_group_dir)\n",
    "    other_ids = ids.copy()\n",
    "    random.shuffle(ids)\n",
    "    random.shuffle(other_ids)\n",
    "\n",
    "    irrelevant, false_positives, relevant, true_positives = 0, 0, 0, 0\n",
    "    # For performance reasons, only choose 5 authors\n",
    "    for id in ids[:10]:\n",
    "        profile.reset()\n",
    "        texts = file_texts(id)\n",
    "\n",
    "        first_text = texts[0]\n",
    "        other_texts = texts[1:]\n",
    "\n",
    "        # This author's first essay is used to profile that author\n",
    "        profile.feed(first_text)\n",
    "\n",
    "        # The rest of their essays are tested against this profile\n",
    "        other_scores = torch.tensor([profile.score(text) for text in other_texts])\n",
    "\n",
    "        irrelevant += len(texts)\n",
    "        false_positives += sum(other_scores < cutoff)\n",
    "\n",
    "        # Compare this author to 20 random different authors\n",
    "        new_relevant, new_true_positives = grade_others(profile, other_ids[:30], id, cutoff)\n",
    "        relevant += new_relevant\n",
    "        true_positives += new_true_positives\n",
    "\n",
    "    # Precision is relevant selections out of all selections\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    # Recall is all relevant selections out of all relevant items\n",
    "    recall = true_positives / relevant\n",
    "\n",
    "    false_negatives = relevant - true_positives\n",
    "\n",
    "    # False positives and false negatives sum to the error count, and\n",
    "    # irrelevant and relevant items sum to the whole set\n",
    "    error_rate = false_negatives + false_positives / (irrelevant + relevant)\n",
    "\n",
    "    return float(precision), float(recall), float(error_rate)\n",
    "\n",
    "\n",
    "def file_texts(id):\n",
    "    filenames = os.listdir(join(bawe_group_dir, id))\n",
    "    texts = []\n",
    "    for filename in filenames:\n",
    "        with open(join(bawe_group_dir, f'{id}/{filename}'), 'r') as f:\n",
    "            texts.append(f.read())\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def grade_others(profile, ids, id, cutoff):\n",
    "    rest_ids = [other_id for other_id in ids if other_id != id]\n",
    "\n",
    "    relevant, true_positives = 0, 0\n",
    "    for other_id in tqdm(rest_ids):\n",
    "        texts = file_texts(other_id)\n",
    "\n",
    "        scores = torch.tensor([profile.score(text) for text in texts])\n",
    "\n",
    "        true_positives += sum(scores < cutoff)\n",
    "        relevant += len(texts)\n",
    "\n",
    "    return relevant, true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 30/30 [02:02<00:00,  4.09s/it]\n",
      "100%|##########| 30/30 [02:05<00:00,  4.18s/it]\n",
      "100%|##########| 30/30 [02:06<00:00,  4.22s/it]\n",
      "100%|##########| 30/30 [02:04<00:00,  4.14s/it]\n",
      "100%|##########| 30/30 [02:04<00:00,  4.16s/it]\n",
      "100%|##########| 30/30 [02:07<00:00,  4.24s/it]\n",
      "100%|##########| 30/30 [02:13<00:00,  4.45s/it]\n",
      "100%|##########| 30/30 [02:13<00:00,  4.44s/it]\n",
      "100%|##########| 30/30 [02:12<00:00,  4.41s/it]\n",
      "100%|##########| 30/30 [02:08<00:00,  4.28s/it]\n"
     ]
    }
   ],
   "source": [
    "pospca_extractor = POSPCAExtractor(4, 10)\n",
    "pospca_profile = MahalanobisProfile(pospca_extractor)\n",
    "\n",
    "pospca_precision, pospca_recall, pospca_error = test_profile(pospca_profile, 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 30/30 [00:36<00:00,  1.21s/it]\n",
      "100%|##########| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|##########| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|##########| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|##########| 30/30 [00:34<00:00,  1.14s/it]\n",
      "100%|##########| 30/30 [00:34<00:00,  1.16s/it]\n",
      "100%|##########| 30/30 [00:34<00:00,  1.15s/it]\n",
      "100%|##########| 30/30 [00:35<00:00,  1.17s/it]\n",
      "100%|##########| 30/30 [00:35<00:00,  1.17s/it]\n",
      "100%|##########| 29/29 [00:34<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "heuristics_extractor = HeuristicExtractor(4)\n",
    "heuristics_profile = MahalanobisProfile(heuristics_extractor)\n",
    "\n",
    "heuristics_precision, heuristics_recall, heuristics_error = test_profile(heuristics_profile, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>POSPCA</th>\n",
       "      <td>0.979986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.019877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Heuristics</th>\n",
       "      <td>0.977012</td>\n",
       "      <td>0.742653</td>\n",
       "      <td>324.016846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Precision    Recall       Error\n",
       "POSPCA       0.979986  1.000000    0.019877\n",
       "Heuristics   0.977012  0.742653  324.016846"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_data = [\n",
    "    [pospca_precision, pospca_recall, pospca_error],\n",
    "    [heuristics_precision, heuristics_recall, heuristics_error]\n",
    "]\n",
    "\n",
    "pd.DataFrame(metric_data, columns=['Precision', 'Recall', 'Error'], index=['POSPCA', 'Heuristics'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
