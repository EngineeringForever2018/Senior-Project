{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divine-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_root = Path('../..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "from notebooks.utils import init_data_dir  # noqa\n",
    "\n",
    "# from notebooks.benchmarking import benchmark_profiles  # noqa\n",
    "\n",
    "init_data_dir(project_root)\n",
    "\n",
    "preprocess_path = join(project_root, Path('data/preprocess'))\n",
    "outputs_path = join(project_root, 'outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-postage",
   "metadata": {},
   "source": [
    "# Progress Report 2\n",
    "\n",
    "Much of the data preprocessing code was refactored so that different datasets could be quickly trained/evaluated on and so that the amount of sentences in a text group could be quickly modified. Benchmarks were computed for each of the profile methods using the new preprocessing functionality. Plans have been made to create a detailed report screen where the sentences that are most suspicious are highlighted.\n",
    "\n",
    "## Benchmarks\n",
    "\n",
    "The dataset was split into groups of 50 sentences each, with the author and the ID of the text attached to each group. Each of the profiles were tested on 10 samples of 30 authors (profile/benchmark code is very slow right now and needs to be optimized so that we can more easily test on larger sets). Out of each set of 30 authors, one author was chosen to be the \"profile\" author, and the others were chosen to be suspects. Out of the texts for the profile author, one was chosen to be a suspect and the rest were fed into the profile. The profile was then tested on all the suspects, and the true/false positives and negatives were collected. We score the profiles based on sensitivity, specificity, balanced accuracy, and precision.\n",
    "\n",
    "POSPCA is performing at chance accuracy (but this seems to be due to a bug that causes it to flag everything). The heuristics are performing the best at ~75% balanced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selected-default",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced accuracy</th>\n",
       "      <th>specificity</th>\n",
       "      <th>sensitivity (recall)</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pospca</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heuristics</th>\n",
       "      <td>0.757503</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.815005</td>\n",
       "      <td>0.99125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            balanced accuracy  specificity  sensitivity (recall)  precision\n",
       "pospca               0.500000          0.0              1.000000    1.00000\n",
       "heuristics           0.757503          0.7              0.815005    0.99125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results = pd.read_hdf(join(outputs_path,\n",
    "                                     'bawe_train_benchmarks.hdf5'))\n",
    "\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-variety",
   "metadata": {},
   "source": [
    "## Explainability and the Detailed Report Screen\n",
    "\n",
    "Our current idea for model explainability is to architecture each profile to work on a sentence level and then use the average of many sentence vectors to profile. This way we can analyze the difference between each sentence in a suspect text compared to the profile. If a sentence is very far from the profile, we can highlight that sentence based on how far it is, indicating that this sentence is part of the reason that the suspect text was flagged. For a profile that is more interpretable, such as the heuristics, we could display additional information on mouseover, such as whether the sentence has more punctuation than usual. If the models are not able to work accurately enough on a sentence level, we could group sentences and employ the same process by highlighting groups of sentences. Smaller groups are preferred for usability.\n",
    "\n",
    "![](essay_no_numbers.png)\n",
    "![](wireframe1.png)\n",
    "![](wireframe2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:avpd]",
   "language": "python",
   "name": "conda-env-avpd-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
