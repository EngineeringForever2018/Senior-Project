{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_corpus_path = Path('../data/bawe/CORPUS_TXT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserve 20% of the essays for a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "dir_contents = listdir(txt_corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_contents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_files = [f for f in dir_contents if isfile(join(txt_corpus_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "random.shuffle(only_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_files) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = only_files[:-184]\n",
    "testing_files = only_files[-184:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files[-5:]\n",
    "testing_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump({'train': training_files, 'test': testing_files}, open(Path('../data/bawe_splits.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('When I was a boy, I fell down. Then I got back up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [[token.pos_ for token in sent] for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(7, 5, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sent_len(pos_text, nlp_=nlp):\n",
    "    sent_lens = [len(sent) for sent in pos_text]\n",
    "    \n",
    "    return max(sent_lens)\n",
    "\n",
    "def find_pos_set(pos_text, nlp_=nlp):\n",
    "    sets = [set(sent) for sent in pos_text]\n",
    "    \n",
    "    result = set()\n",
    "    \n",
    "    for set_ in sets:\n",
    "        result |= set_\n",
    "    \n",
    "    return result\n",
    "\n",
    "def pos_tag(text, nlp_=nlp):\n",
    "    sents = [sent for sent in nlp_(text).sents]\n",
    "    \n",
    "    return [[token.pos_ for token in sent] for sent in sents]\n",
    "\n",
    "def pos_tag_file(f, nlp_=nlp):\n",
    "    text = open(join(txt_corpus_path, Path(f))).read()\n",
    "    \n",
    "    return pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pickle.load(open(Path('../data/bawe_splits.p'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = splits['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2577/2577 [14:44<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "max_sent_lens = []\n",
    "pos_set = set()\n",
    "for f in tqdm(training_files):\n",
    "    text = open(join(txt_corpus_path, Path(f))).read()\n",
    "    \n",
    "    pos_text = pos_tag(text)\n",
    "    \n",
    "    max_sent_lens.append(find_max_sent_len(pos_text))\n",
    "    pos_set |= find_pos_set(pos_text)\n",
    "\n",
    "max_sent_len = max(max_sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'SPACE',\n",
       " 'SYM',\n",
       " 'VERB'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_counts = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vocab = Vocab(Counter(pos_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab['asf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'max_sent_len': max_sent_len, 'pos_vocab': pos_vocab}, open('../data/bawe_train_stats.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bawe_train_stats = pickle.load(open('../data/bawe_train_stats.p', 'rb'))\n",
    "max_sent_len = bawe_train_stats['max_sent_len']\n",
    "pos_vocab = bawe_train_stats['pos_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(pos_text, par_len=4, max_sent_len_=max_sent_len):\n",
    "    sent_tensors = []\n",
    "    sent_lens = []\n",
    "    \n",
    "    for sent in pos_text:\n",
    "        sent_tensor, sent_len = sent_to_tensor(sent, max_sent_len_)\n",
    "        \n",
    "        sent_tensors.append(sent_tensor.unsqueeze(0))\n",
    "        sent_lens.append(sent_len)\n",
    "    \n",
    "    sent_count = len(sent_tensors)\n",
    "    new_sent_count = sent_count - (sent_count % par_len)\n",
    "    \n",
    "    tensor = torch.cat(sent_tensors, dim=0)[:new_sent_count]\n",
    "    lens = torch.cat(sent_lens, dim=0)[:new_sent_count]\n",
    "    \n",
    "    tensor = torch.reshape(tensor, [-1, 4, max_sent_len_])\n",
    "    lens = torch.reshape(lens, [-1, 4])\n",
    "    \n",
    "    return tensor, lens\n",
    "\n",
    "def sent_to_tensor(sent, max_sent_len_=max_sent_len, pos_vocab_=pos_vocab):\n",
    "    pos_indices = [pos_vocab_[pos] for pos in sent]\n",
    "    sent_len = len(pos_indices)\n",
    "    \n",
    "    tensor = torch.full([max_sent_len_], pos_vocab_['<pad>'])\n",
    "    \n",
    "    non_padded = torch.tensor(pos_indices)\n",
    "    \n",
    "    tensor[:sent_len] = non_padded\n",
    "    \n",
    "    return tensor, torch.tensor([sent_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt_corpus_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0662e14e2a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1872f0d5ce4d>\u001b[0m in \u001b[0;36mpos_tag_file\u001b[0;34m(f, nlp_)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_tag_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_corpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'txt_corpus_path' is not defined"
     ]
    }
   ],
   "source": [
    "pos_text = pos_tag_file(training_files[0])\n",
    "\n",
    "to_tensor(pos_text)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2577/2577 [10:20<00:00,  4.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        tensor = torch.reshape(tensor, [-1, 4, max_sent_len])\n",
    "        sent_lens = torch.reshape(sent_lens, [-1, 4])labels_tensors = []\n",
    "\n",
    "for f in tqdm(training_files):\n",
    "    pos_text = pos_tag_file(f)\n",
    "    \n",
    "    tensor, sent_lens = to_tensor(pos_text, 4, max_sent_len)\n",
    "    \n",
    "    label = int(f[:4])\n",
    "    \n",
    "    labels_tensors.append((label, tensor, sent_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensors.sort(key=lambda label_tensor: label_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 48, 33, 36],\n",
       "        [71, 42, 36, 81],\n",
       "        [21, 11, 55, 55],\n",
       "        [80, 20, 39, 42],\n",
       "        [ 9, 51, 32, 62],\n",
       "        [29, 39, 36, 55],\n",
       "        [15, 29, 44, 34],\n",
       "        [31, 56, 43, 44],\n",
       "        [74, 39, 35, 24],\n",
       "        [ 8, 27, 19, 31],\n",
       "        [40, 48, 31, 14],\n",
       "        [36, 23, 27, 52],\n",
       "        [42,  6, 11, 35],\n",
       "        [31, 37, 25, 29],\n",
       "        [45, 30, 51, 49],\n",
       "        [48, 48, 44, 20],\n",
       "        [19, 52, 51, 42]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensors[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels_tensors = [(torch.full([len(sent_lens)], label), tensor, sent_lens) for label, tensor, sent_lens in labels_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list, tensor_list, sent_lens_list = zip(*new_labels_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.cat(labels_list, dim=0).contiguous()\n",
    "tensor = torch.cat(tensor_list, dim=0).contiguous()\n",
    "sent_lens = torch.cat(sent_lens_list, dim=0).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39, 48, 33, 36],\n",
       "        [71, 42, 36, 81],\n",
       "        [21, 11, 55, 55],\n",
       "        [80, 20, 39, 42],\n",
       "        [ 9, 51, 32, 62]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labels, '../data/bawe-preprocess/labels.pt')\n",
    "torch.save(tensor, '../data/bawe-preprocess/data.pt')\n",
    "torch.save(sent_lens, '../data/bawe-preprocess/sent_lens.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93069"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = set([int(label) for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(54)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_counts = {int(label): sum(labels == label) for label in tqdm(label_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set_list = list(label_set)\n",
    "label_set_list.sort()\n",
    "\n",
    "position = 0\n",
    "label_starts = {}\n",
    "for label in label_set_list:\n",
    "    label_starts[label] = position\n",
    "    position += int(label_counts[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ends = {label: label_starts[label] + int(label_counts[label]) for label in label_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(label_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93069"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ends[6998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'label_counts': label_counts, 'label_starts': label_starts, 'label_ends': label_ends}, open('../data/bawe-preprocess/label_stats.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "22 - (22 % 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
