{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dev = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "writer = SummaryWriter(f'runs/{datetime.now()}-bawe-par-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "labels = torch.load('../data/bawe-preprocess/labels.pt')\n",
    "data = torch.load('../data/bawe-preprocess/data.pt')\n",
    "sent_lens = torch.load('../data/bawe-preprocess/sent_lens.pt')\n",
    "\n",
    "label_set = pickle.load(open('../data/bawe-preprocess/label_set.p', 'rb'))\n",
    "bawe_train_stats = pickle.load(open('../data/bawe_train_stats.p', 'rb'))\n",
    "max_sent_len = bawe_train_stats['max_sent_len']\n",
    "pos_vocab = bawe_train_stats['pos_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "order = torch.randperm(len(labels))\n",
    "\n",
    "valid_size = int(len(order) / 5)\n",
    "\n",
    "valid_indices = order[:valid_size]\n",
    "\n",
    "valid_selection = torch.zeros(len(labels), dtype=bool)\n",
    "valid_selection[valid_indices] = True\n",
    "\n",
    "train_selection = torch.logical_not(valid_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_labels = labels[train_selection].contiguous()\n",
    "train_data = data[train_selection].contiguous()\n",
    "train_sent_lens = sent_lens[train_selection].contiguous()\n",
    "\n",
    "valid_labels = labels[valid_selection].contiguous()\n",
    "valid_data = data[valid_selection].contiguous()\n",
    "valid_sent_lens = sent_lens[valid_selection].contiguous()\n",
    "\n",
    "train_label_set = sorted(list(set([int(label) for label in train_labels])))\n",
    "valid_label_set = sorted(list(set([int(label) for label in valid_labels])))\n",
    "\n",
    "train_label_set = torch.tensor(train_label_set)\n",
    "valid_label_set = torch.tensor(valid_label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(train_data, train_sent_lens, train_labels)\n",
    "valid_set = TensorDataset(valid_data, valid_sent_lens, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class EqualOpDataLoader:\n",
    "    def __init__(self, dataset, label_set, bs=1):\n",
    "        self.dataset = dataset\n",
    "        _, _, self.labels = dataset[:]\n",
    "        self.label_set = label_set\n",
    "        self.bs = bs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        bs = self.bs\n",
    "        for _ in range(int(len(self.dataset) / bs)):\n",
    "            order = torch.randperm(len(self.label_set))\n",
    "            \n",
    "            first_labels = self.label_set[order[:bs]]\n",
    "            second_labels = first_labels\n",
    "            \n",
    "            different_selection = torch.bernoulli(torch.full([bs], 0.5)).to(bool)\n",
    "            different_labels = self.label_set[order[bs:2*bs]]\n",
    "            \n",
    "            second_labels[different_selection] = different_labels[different_selection]\n",
    "            \n",
    "            def sample_label(label):\n",
    "                selection = (self.labels == label)\n",
    "                \n",
    "                dataset_selection = self.dataset[selection]\n",
    "                \n",
    "                data_, sent_lens_, labels_ = dataset_selection\n",
    "                \n",
    "                selected_index = torch.randint(high=len(labels_), size=(1,))\n",
    "                \n",
    "                return data_[selected_index:selected_index+1], sent_lens_[selected_index:selected_index+1], labels_[selected_index:selected_index+1]\n",
    "            \n",
    "            def samples(chosen_labels):\n",
    "                result = [sample_label(label) for label in chosen_labels]\n",
    "                \n",
    "                data_, sent_lens_, labels_ = zip(*result)\n",
    "                \n",
    "                data_ = torch.cat(data_, dim=0)\n",
    "                sent_lens_ = torch.cat(sent_lens_, dim=0)\n",
    "                labels_ = torch.cat(labels_, dim=0)\n",
    "                \n",
    "                return data_, sent_lens_, labels_\n",
    "            \n",
    "            first_data, first_sent_lens, first_labels = samples(first_labels)\n",
    "            second_data, second_sent_lens, second_labels = samples(second_labels)\n",
    "            \n",
    "            yield first_data, first_sent_lens, first_labels, second_data, second_sent_lens, second_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset) / self.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class GPUDataLoader:\n",
    "    def __init__(self, dl, embedding, dev):\n",
    "        self.dl = dl\n",
    "        self.embedding = embedding\n",
    "        self.dev = dev\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for first_data, first_sent_lens, first_labels, second_data, second_sent_lens, second_labels in self.dl:\n",
    "            first_data, second_data = self.embedding(first_data).to(self.dev), self.embedding(second_data).to(self.dev)\n",
    "            \n",
    "            def reform(data_, sent_lens_):\n",
    "                batch_size, _, seq_len, embedding_dim = data_.size()\n",
    "                \n",
    "                # (4 * batch_size, seq_len, embedding_dim)\n",
    "                reshaped = torch.reshape(data_, [-1, seq_len, embedding_dim])\n",
    "                \n",
    "                return torch.transpose(reshaped, 0, 1), torch.reshape(sent_lens_, [-1])\n",
    "            \n",
    "            first_data, first_sent_lens = reform(first_data, first_sent_lens)\n",
    "            second_data, second_sent_lens = reform(second_data, second_sent_lens)\n",
    "            \n",
    "            first_data = pack_padded_sequence(first_data, first_sent_lens, enforce_sorted=False)\n",
    "            second_data = pack_padded_sequence(second_data, second_sent_lens, enforce_sorted=False)\n",
    "            \n",
    "            yield first_data, first_labels, second_data, second_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear, LSTM, Module\n",
    "from torch.nn.functional import softmax\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, PackedSequence\n",
    "\n",
    "class BahdanauAttention(Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_module = Linear(encoding_dim, encoding_dim, bias=False)\n",
    "        self.alignment_module = Linear(encoding_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, encoder_outputs):\n",
    "        # (seq_len, batch_size, encoder_size) -> (batch_size, seq_len, encoder_size)\n",
    "        encoder_outputs = torch.transpose(encoder_outputs, 0, 1)\n",
    "        \n",
    "        # (batch_size, seq_len, encoder_size)\n",
    "        encoder_activations = (self.encoder_module(encoder_outputs))\n",
    "        # (batch_size, seq_len, encoder_size) -> (batch_size, seq_len, 1)\n",
    "        alignment_scores = self.alignment_module(torch.tanh(encoder_activations))\n",
    "        \n",
    "        attn_weights = softmax(alignment_scores, dim=1)\n",
    "        \n",
    "        # (batch_size, encoder_size, seq_len)\n",
    "        encoder_outputs = torch.transpose(encoder_outputs, 1, 2)\n",
    "        # (batch_size, encoder_size, seq_len) X (batch_size, seq_len, 1) -> (batch_size, encoder_size, 1)\n",
    "        context_vectors = torch.bmm(encoder_outputs, attn_weights)\n",
    "        \n",
    "        return context_vectors.squeeze(2)\n",
    "\n",
    "class SentenceEncoder(Module):\n",
    "    def __init__(self, embedding_dim, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = LSTM(embedding_dim, encoding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoding, _= self.encoder(x)\n",
    "        \n",
    "        if isinstance(encoding, PackedSequence):\n",
    "            encoding, sent_lens = pad_packed_sequence(encoding)\n",
    "\n",
    "            encoding = encoding[sent_lens - 1, torch.arange(encoding.shape[1]), :]\n",
    "        else:\n",
    "            encoding = encoding[-1]\n",
    "\n",
    "        return torch.reshape(encoding, [4, -1, encoding.shape[1]])\n",
    "\n",
    "class ParEncoder(Module):\n",
    "    def __init__(self, sent_encoding_dim, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = LSTM(sent_encoding_dim, encoding_dim)\n",
    "        self.attention = BahdanauAttention(encoding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoding, _ = self.encoder(x)\n",
    "        \n",
    "        return self.attention(encoding)\n",
    "\n",
    "class StyleEncoder(Module):\n",
    "    def __init__(self, sentence_encoder, par_encoder):\n",
    "        super().__init__()\n",
    "        self.sentence_encoder = sentence_encoder\n",
    "        self.par_encoder = par_encoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sentence_encoding = self.sentence_encoder(x)\n",
    "        \n",
    "        return self.par_encoder(sentence_encoding)\n",
    "\n",
    "class EuclideanDiscriminator(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        diff = x1 - x2\n",
    "        \n",
    "        distance = torch.sqrt(torch.sum(diff * diff, dim=1))\n",
    "        \n",
    "        probability = torch.sigmoid(self.linear(distance.unsqueeze(1)))\n",
    "        \n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "\n",
    "embedding = Embedding(len(pos_vocab), 10, padding_idx=pos_vocab['<pad>'])\n",
    "sentence_encoder = SentenceEncoder(10, 10).to(dev)\n",
    "par_encoder = ParEncoder(10, 5).to(dev)\n",
    "\n",
    "style_encoder = StyleEncoder(sentence_encoder, par_encoder).to(dev)\n",
    "style_discriminator = EuclideanDiscriminator().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(Module):\n",
    "    def __init__(self, style_encoder_, style_discriminator_):\n",
    "        super().__init__()\n",
    "        self.style_encoder = style_encoder_\n",
    "        self.style_discriminator = style_discriminator_\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1_encoding = self.style_encoder(x1)\n",
    "        x2_encoding = self.style_encoder(x2)\n",
    "        \n",
    "        return self.style_discriminator(x1_encoding, x2_encoding)\n",
    "\n",
    "combined = CombinedModel(style_encoder, style_discriminator)\n",
    "    \n",
    "writer.add_graph(combined, (torch.zeros([24, 80, 10]).to(dev), torch.zeros([24, 80, 10]).to(dev)))\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.seed()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 1\n",
    "lr = 1e-3\n",
    "opt = optim.Adam([{'params': style_discriminator.parameters()}, {'params': style_encoder.parameters()}, {'params': embedding.parameters()}])\n",
    "criterion = mse_loss\n",
    "bs = 75\n",
    "\n",
    "train_dl = GPUDataLoader(EqualOpDataLoader(train_set, train_label_set, bs=bs), embedding, dev)\n",
    "valid_dl = GPUDataLoader(EqualOpDataLoader(valid_set, valid_label_set, bs=bs), embedding, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.seed()\n",
    "\n",
    "\n",
    "def fit(validate=True):\n",
    "    for epoch in range(epochs):\n",
    "        for index, (x1b, y1b, x2b, y2b) in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "            x1_encoding = style_encoder(x1b)\n",
    "            x2_encoding = style_encoder(x2b)\n",
    "\n",
    "            pred = style_discriminator(x1_encoding, x2_encoding).squeeze(1)\n",
    "\n",
    "            yb = y_difference(y1b, y2b).to(dtype=torch.float)\n",
    "\n",
    "            loss = criterion(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            writer.add_scalar('Training Loss', loss, index)\n",
    "            writer.flush()\n",
    "\n",
    "            if index % 100 == 0:\n",
    "                valid_loss = evaluate(valid_dl)\n",
    "                writer.add_scalar('Validation Loss', valid_loss, index)\n",
    "\n",
    "            if index == 900:\n",
    "                break\n",
    "        break\n",
    "#         train_loss = evaluate(train_eval_dl)\n",
    "#         writer.add_scalar('Training Loss', train_loss, epoch)\n",
    "        if validate:\n",
    "            valid_loss = evaluate(valid_dl)\n",
    "            writer.add_scalar('Validation Loss', valid_loss, epoch)\n",
    "#             writer.add_scalar('Validation Accuracy', valid_accuracy, epoch)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "def y_difference(y1, y2):\n",
    "    return torch.logical_not((y1 == y2)).to(dtype=int).to(dev)\n",
    "\n",
    "\n",
    "def evaluate(dl, give_acc=False):\n",
    "    with torch.no_grad():\n",
    "        preds_y = [(style_discriminator(style_encoder(x1b),\n",
    "                                        style_encoder(x2b)),\n",
    "                    y_difference(y1b, y2b))\n",
    "                   for x1b, y1b, x2b, y2b in dl]\n",
    "\n",
    "        losses = [criterion(preds_b.squeeze(1), yb) for preds_b, yb in preds_y]\n",
    "        loss = sum(losses) / len(losses)\n",
    "\n",
    "        if give_acc:\n",
    "            accs = [accuracy(preds_b, yb) for preds_b, yb in preds_y]\n",
    "            acc = sum(accs) / len(accs)\n",
    "\n",
    "            return loss, acc\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def accuracy(out, y):\n",
    "    preds = out > 0.5\n",
    "    return (preds == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(embedding.state_dict(), '../resources/bawe_embedding_sd.pt')\n",
    "# torch.save(style_encoder.state_dict(), '../resources/bawe_style_encoder_sd.pt')\n",
    "# torch.save(style_discriminator.state_dict(), '../resources/bawe_style_discriminator_sd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
